{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CartPole Env.\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 10                 # 히든 레이어의 뉴런 수\n",
    "batch_size = 5         # 5번을 실행하고 그 중에서 하나를 고른다.\n",
    "learning_rate = 1e-2   # 학습률\n",
    "                       # 학습률을 높게 하면 시스템이 데이터에 빠르게 적응하지만 \n",
    "                       # 예전 데이터를 금방 잊어버릴 것입니다\n",
    "                       # 학습률이 낮으면 더 느리게 학습됨. \n",
    "                       # 하지만 새로운 데이터에 있는 잡음이나 대표성 없는 데이터 포인트에 덜 민감해집니다.\n",
    "gamma = 0.99           # discount factor for reward\n",
    "                       # 감마가 0이라면 오직 다음 시간의 보상만을 고려한다.(빠르게 최적의 행동 결정 가능)\n",
    "                       # 감마가 1이라면 바로 앞의 보상뿐만 아니라 미래의 보상도 고려한다.\n",
    "D = 4                  # 입력값의 갯수\n",
    "                       # x : position of cart on the track\n",
    "                       # θ : angle of the pole with the vertical\n",
    "                       # dx/dt : cart velocity\n",
    "                       # dθ/dt : rate of change of the angle\n",
    "                \n",
    "                       # output으로 [1,0] 또는 [0,1]이 나오는데 이는 오른쪽으로 움직여라 or 왼쪽으로 움직이라는 뜻."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0204 20:54:50.113221 11072 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 네트워크를 만들기 위해 필요한 요소들을 선언해 놓는다.\n",
    "# (output이 1,0 또는 0,1이 나올 수 있도록 도와주는 요소라고 생각하면 편하다.)\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "# 처음에는 input layer에서 4개의 입력값이 첫번째 히든레이어(10개의 뉴런을 가짐)로 들어간다.\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "# layer 1의 activation 함수는 Lelu이다.\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "# 그 다음에는 다시한번 히든레이어1의 결과값들을 히든레이어2에 집어넣고 1개의 output으로 뽑아낸다.\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "score = tf.matmul(layer1,W2)\n",
    "\n",
    "# Neural Network를 Non-Linear하게 만들어주기 위한 Activation funtion.\n",
    "# 'W2'에서 action에 대한 결과를 0~1로 만들어 주기 위해서 sigmoid를 이용.\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# good policy를 얻기 위한 네트워크를 정의\n",
    "tvars = tf.trainable_variables()                                 # weight\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")    # output : Prob.\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# loss funtion으로 가중치를 좋은 이익을 주는 행위와 가능성이 낮지 않은 행동을 하는 방향으로 가중치를 보낸다. \n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars) # 새로운 정책을 얻기위한 gradient를 얻는다.\n",
    "\n",
    "# 여러 에피소드에서 일련의 기울기를 수집한다.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") # Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward를 1차원배열로 만들어 gamma를 통해 깎인 보상을 계산하여 그 결과를 리턴값으로 주는 함수\n",
    "# 편의를 위한 도우미 함수임.\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 12.200.  Total average reward 12.200\n",
      "Average reward for episode 23.800.  Total average reward 12.316\n",
      "Average reward for episode 13.000.  Total average reward 12.323\n",
      "Average reward for episode 15.800.  Total average reward 12.358\n",
      "Average reward for episode 16.600.  Total average reward 12.400\n",
      "Average reward for episode 19.800.  Total average reward 12.474\n",
      "Average reward for episode 18.800.  Total average reward 12.537\n",
      "Average reward for episode 23.000.  Total average reward 12.642\n",
      "Average reward for episode 23.800.  Total average reward 12.754\n",
      "Average reward for episode 25.200.  Total average reward 12.878\n",
      "Average reward for episode 12.200.  Total average reward 12.871\n",
      "Average reward for episode 20.000.  Total average reward 12.942\n",
      "Average reward for episode 31.200.  Total average reward 13.125\n",
      "Average reward for episode 17.400.  Total average reward 13.168\n",
      "Average reward for episode 24.400.  Total average reward 13.280\n",
      "Average reward for episode 23.000.  Total average reward 13.377\n",
      "Average reward for episode 15.800.  Total average reward 13.402\n",
      "Average reward for episode 24.800.  Total average reward 13.516\n",
      "Average reward for episode 27.200.  Total average reward 13.652\n",
      "Average reward for episode 31.000.  Total average reward 13.826\n",
      "Average reward for episode 18.800.  Total average reward 13.876\n",
      "Average reward for episode 17.600.  Total average reward 13.913\n",
      "Average reward for episode 20.400.  Total average reward 13.978\n",
      "Average reward for episode 27.800.  Total average reward 14.116\n",
      "Average reward for episode 26.400.  Total average reward 14.239\n",
      "Average reward for episode 20.600.  Total average reward 14.302\n",
      "Average reward for episode 31.000.  Total average reward 14.469\n",
      "Average reward for episode 40.000.  Total average reward 14.725\n",
      "Average reward for episode 32.400.  Total average reward 14.901\n",
      "Average reward for episode 23.600.  Total average reward 14.988\n",
      "Average reward for episode 22.600.  Total average reward 15.065\n",
      "Average reward for episode 33.000.  Total average reward 15.244\n",
      "Average reward for episode 21.200.  Total average reward 15.303\n",
      "Average reward for episode 26.400.  Total average reward 15.414\n",
      "Average reward for episode 26.800.  Total average reward 15.528\n",
      "Average reward for episode 26.200.  Total average reward 15.635\n",
      "Average reward for episode 36.200.  Total average reward 15.841\n",
      "Average reward for episode 38.800.  Total average reward 16.070\n",
      "Average reward for episode 25.200.  Total average reward 16.162\n",
      "Average reward for episode 28.200.  Total average reward 16.282\n",
      "Average reward for episode 31.000.  Total average reward 16.429\n",
      "Average reward for episode 36.000.  Total average reward 16.625\n",
      "Average reward for episode 39.000.  Total average reward 16.849\n",
      "Average reward for episode 27.800.  Total average reward 16.958\n",
      "Average reward for episode 18.800.  Total average reward 16.976\n",
      "Average reward for episode 49.000.  Total average reward 17.297\n",
      "Average reward for episode 29.000.  Total average reward 17.414\n",
      "Average reward for episode 29.400.  Total average reward 17.534\n",
      "Average reward for episode 40.000.  Total average reward 17.758\n",
      "Average reward for episode 40.800.  Total average reward 17.989\n",
      "Average reward for episode 33.400.  Total average reward 18.143\n",
      "Average reward for episode 36.600.  Total average reward 18.327\n",
      "Average reward for episode 32.000.  Total average reward 18.464\n",
      "Average reward for episode 24.600.  Total average reward 18.525\n",
      "Average reward for episode 28.200.  Total average reward 18.622\n",
      "Average reward for episode 47.200.  Total average reward 18.908\n",
      "Average reward for episode 25.800.  Total average reward 18.977\n",
      "Average reward for episode 39.800.  Total average reward 19.185\n",
      "Average reward for episode 48.400.  Total average reward 19.477\n",
      "Average reward for episode 32.000.  Total average reward 19.603\n",
      "Average reward for episode 36.600.  Total average reward 19.772\n",
      "Average reward for episode 44.200.  Total average reward 20.017\n",
      "Average reward for episode 27.800.  Total average reward 20.095\n",
      "Average reward for episode 53.400.  Total average reward 20.428\n",
      "Average reward for episode 43.200.  Total average reward 20.655\n",
      "Average reward for episode 34.600.  Total average reward 20.795\n",
      "Average reward for episode 25.400.  Total average reward 20.841\n",
      "Average reward for episode 49.200.  Total average reward 21.124\n",
      "Average reward for episode 33.200.  Total average reward 21.245\n",
      "Average reward for episode 44.800.  Total average reward 21.481\n",
      "Average reward for episode 41.000.  Total average reward 21.676\n",
      "Average reward for episode 58.800.  Total average reward 22.047\n",
      "Average reward for episode 77.600.  Total average reward 22.603\n",
      "Average reward for episode 40.200.  Total average reward 22.779\n",
      "Average reward for episode 55.400.  Total average reward 23.105\n",
      "Average reward for episode 51.600.  Total average reward 23.390\n",
      "Average reward for episode 44.200.  Total average reward 23.598\n",
      "Average reward for episode 43.800.  Total average reward 23.800\n",
      "Average reward for episode 59.200.  Total average reward 24.154\n",
      "Average reward for episode 48.800.  Total average reward 24.400\n",
      "Average reward for episode 47.800.  Total average reward 24.634\n",
      "Average reward for episode 48.000.  Total average reward 24.868\n",
      "Average reward for episode 40.200.  Total average reward 25.021\n",
      "Average reward for episode 60.600.  Total average reward 25.377\n",
      "Average reward for episode 38.000.  Total average reward 25.503\n",
      "Average reward for episode 43.600.  Total average reward 25.684\n",
      "Average reward for episode 48.800.  Total average reward 25.916\n",
      "Average reward for episode 68.200.  Total average reward 26.338\n",
      "Average reward for episode 30.800.  Total average reward 26.383\n",
      "Average reward for episode 62.400.  Total average reward 26.743\n",
      "Average reward for episode 70.000.  Total average reward 27.176\n",
      "Average reward for episode 49.800.  Total average reward 27.402\n",
      "Average reward for episode 37.000.  Total average reward 27.498\n",
      "Average reward for episode 57.600.  Total average reward 27.799\n",
      "Average reward for episode 50.400.  Total average reward 28.025\n",
      "Average reward for episode 53.400.  Total average reward 28.279\n",
      "Average reward for episode 46.600.  Total average reward 28.462\n",
      "Average reward for episode 68.800.  Total average reward 28.865\n",
      "Average reward for episode 96.600.  Total average reward 29.543\n",
      "Average reward for episode 96.200.  Total average reward 30.209\n",
      "Average reward for episode 62.200.  Total average reward 30.529\n",
      "Average reward for episode 68.400.  Total average reward 30.908\n",
      "Average reward for episode 53.200.  Total average reward 31.131\n",
      "Average reward for episode 110.200.  Total average reward 31.921\n",
      "Average reward for episode 68.000.  Total average reward 32.282\n",
      "Average reward for episode 102.400.  Total average reward 32.983\n",
      "Average reward for episode 114.400.  Total average reward 33.798\n",
      "Average reward for episode 89.200.  Total average reward 34.352\n",
      "Average reward for episode 69.800.  Total average reward 34.706\n",
      "Average reward for episode 96.400.  Total average reward 35.323\n",
      "Average reward for episode 84.800.  Total average reward 35.818\n",
      "Average reward for episode 115.600.  Total average reward 36.616\n",
      "Average reward for episode 71.200.  Total average reward 36.962\n",
      "Average reward for episode 53.400.  Total average reward 37.126\n",
      "Average reward for episode 107.000.  Total average reward 37.825\n",
      "Average reward for episode 121.800.  Total average reward 38.664\n",
      "Average reward for episode 106.800.  Total average reward 39.346\n",
      "Average reward for episode 96.600.  Total average reward 39.918\n",
      "Average reward for episode 94.600.  Total average reward 40.465\n",
      "Average reward for episode 159.600.  Total average reward 41.656\n",
      "Average reward for episode 108.000.  Total average reward 42.320\n",
      "Average reward for episode 136.400.  Total average reward 43.261\n",
      "Average reward for episode 135.400.  Total average reward 44.182\n",
      "Average reward for episode 127.800.  Total average reward 45.018\n",
      "Average reward for episode 89.200.  Total average reward 45.460\n",
      "Average reward for episode 116.000.  Total average reward 46.165\n",
      "Average reward for episode 129.200.  Total average reward 46.996\n",
      "Average reward for episode 134.200.  Total average reward 47.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 123.200.  Total average reward 48.621\n",
      "Average reward for episode 135.800.  Total average reward 49.493\n",
      "Average reward for episode 153.400.  Total average reward 50.532\n",
      "Average reward for episode 177.200.  Total average reward 51.799\n",
      "Average reward for episode 153.200.  Total average reward 52.813\n",
      "Average reward for episode 164.600.  Total average reward 53.931\n",
      "Average reward for episode 84.000.  Total average reward 54.231\n",
      "Average reward for episode 181.200.  Total average reward 55.501\n",
      "Average reward for episode 155.400.  Total average reward 56.500\n",
      "Average reward for episode 162.800.  Total average reward 57.563\n",
      "Average reward for episode 135.600.  Total average reward 58.343\n",
      "Average reward for episode 150.200.  Total average reward 59.262\n",
      "Average reward for episode 142.200.  Total average reward 60.091\n",
      "Average reward for episode 142.000.  Total average reward 60.910\n",
      "Average reward for episode 137.400.  Total average reward 61.675\n",
      "Average reward for episode 162.400.  Total average reward 62.683\n",
      "Average reward for episode 133.600.  Total average reward 63.392\n",
      "Average reward for episode 181.200.  Total average reward 64.570\n",
      "Average reward for episode 154.400.  Total average reward 65.468\n",
      "Average reward for episode 182.000.  Total average reward 66.633\n",
      "Average reward for episode 144.000.  Total average reward 67.407\n",
      "Average reward for episode 154.800.  Total average reward 68.281\n",
      "Average reward for episode 200.000.  Total average reward 69.598\n",
      "Average reward for episode 192.400.  Total average reward 70.826\n",
      "Average reward for episode 188.600.  Total average reward 72.004\n",
      "Average reward for episode 183.200.  Total average reward 73.116\n",
      "Average reward for episode 193.000.  Total average reward 74.315\n",
      "Average reward for episode 141.200.  Total average reward 74.984\n",
      "Average reward for episode 178.600.  Total average reward 76.020\n",
      "Average reward for episode 170.000.  Total average reward 76.960\n",
      "Average reward for episode 192.800.  Total average reward 78.118\n",
      "Average reward for episode 187.400.  Total average reward 79.211\n",
      "Average reward for episode 146.800.  Total average reward 79.887\n",
      "Average reward for episode 165.800.  Total average reward 80.746\n",
      "Average reward for episode 157.600.  Total average reward 81.514\n",
      "Average reward for episode 192.800.  Total average reward 82.627\n",
      "Average reward for episode 186.800.  Total average reward 83.669\n",
      "Average reward for episode 145.800.  Total average reward 84.290\n",
      "Average reward for episode 200.000.  Total average reward 85.447\n",
      "Average reward for episode 200.000.  Total average reward 86.593\n",
      "Average reward for episode 165.200.  Total average reward 87.379\n",
      "Average reward for episode 171.000.  Total average reward 88.215\n",
      "Average reward for episode 146.000.  Total average reward 88.793\n",
      "Average reward for episode 200.000.  Total average reward 89.905\n",
      "Average reward for episode 175.800.  Total average reward 90.764\n",
      "Average reward for episode 142.600.  Total average reward 91.282\n",
      "Average reward for episode 184.200.  Total average reward 92.212\n",
      "Average reward for episode 200.000.  Total average reward 93.289\n",
      "Average reward for episode 200.000.  Total average reward 94.357\n",
      "Average reward for episode 196.000.  Total average reward 95.373\n",
      "Average reward for episode 169.400.  Total average reward 96.113\n",
      "Average reward for episode 194.600.  Total average reward 97.098\n",
      "Average reward for episode 188.000.  Total average reward 98.007\n",
      "Average reward for episode 183.400.  Total average reward 98.861\n",
      "Average reward for episode 200.000.  Total average reward 99.872\n",
      "Average reward for episode 172.600.  Total average reward 100.600\n",
      "Average reward for episode 193.400.  Total average reward 101.528\n",
      "Average reward for episode 170.400.  Total average reward 102.216\n",
      "Average reward for episode 198.600.  Total average reward 103.180\n",
      "Average reward for episode 185.000.  Total average reward 103.998\n",
      "Average reward for episode 193.800.  Total average reward 104.897\n",
      "Average reward for episode 179.800.  Total average reward 105.646\n",
      "Average reward for episode 190.800.  Total average reward 106.497\n",
      "Average reward for episode 182.600.  Total average reward 107.258\n",
      "Average reward for episode 200.000.  Total average reward 108.186\n",
      "Average reward for episode 200.000.  Total average reward 109.104\n",
      "Average reward for episode 199.400.  Total average reward 110.007\n",
      "Average reward for episode 200.000.  Total average reward 110.907\n",
      "Average reward for episode 199.200.  Total average reward 111.790\n",
      "Average reward for episode 179.400.  Total average reward 112.466\n",
      "Average reward for episode 190.800.  Total average reward 113.249\n",
      "Average reward for episode 192.400.  Total average reward 114.040\n",
      "Average reward for episode 200.000.  Total average reward 114.900\n",
      "Average reward for episode 200.000.  Total average reward 115.751\n",
      "Average reward for episode 200.000.  Total average reward 116.594\n",
      "Average reward for episode 198.400.  Total average reward 117.412\n",
      "Average reward for episode 169.600.  Total average reward 117.934\n",
      "Average reward for episode 200.000.  Total average reward 118.754\n",
      "Average reward for episode 173.000.  Total average reward 119.297\n",
      "Average reward for episode 200.000.  Total average reward 120.104\n",
      "Average reward for episode 200.000.  Total average reward 120.903\n",
      "Average reward for episode 200.000.  Total average reward 121.694\n",
      "Average reward for episode 200.000.  Total average reward 122.477\n",
      "Average reward for episode 189.400.  Total average reward 123.146\n",
      "Average reward for episode 196.200.  Total average reward 123.876\n",
      "Average reward for episode 196.600.  Total average reward 124.604\n",
      "Average reward for episode 193.200.  Total average reward 125.290\n",
      "Average reward for episode 183.400.  Total average reward 125.871\n",
      "Average reward for episode 185.000.  Total average reward 126.462\n",
      "Average reward for episode 194.800.  Total average reward 127.145\n",
      "Average reward for episode 166.200.  Total average reward 127.536\n",
      "Average reward for episode 186.200.  Total average reward 128.123\n",
      "Average reward for episode 200.000.  Total average reward 128.841\n",
      "Average reward for episode 196.000.  Total average reward 129.513\n",
      "Average reward for episode 200.000.  Total average reward 130.218\n",
      "Average reward for episode 194.800.  Total average reward 130.864\n",
      "Average reward for episode 177.600.  Total average reward 131.331\n",
      "Average reward for episode 200.000.  Total average reward 132.018\n",
      "Average reward for episode 179.800.  Total average reward 132.496\n",
      "Average reward for episode 200.000.  Total average reward 133.171\n",
      "Average reward for episode 200.000.  Total average reward 133.839\n",
      "Average reward for episode 200.000.  Total average reward 134.500\n",
      "Average reward for episode 200.000.  Total average reward 135.155\n",
      "Average reward for episode 200.000.  Total average reward 135.804\n",
      "Average reward for episode 188.400.  Total average reward 136.330\n",
      "Average reward for episode 185.800.  Total average reward 136.825\n",
      "Average reward for episode 195.400.  Total average reward 137.410\n",
      "Average reward for episode 200.000.  Total average reward 138.036\n",
      "Average reward for episode 200.000.  Total average reward 138.656\n",
      "Average reward for episode 200.000.  Total average reward 139.269\n",
      "Average reward for episode 158.600.  Total average reward 139.463\n",
      "Average reward for episode 197.200.  Total average reward 140.040\n",
      "Average reward for episode 200.000.  Total average reward 140.640\n",
      "Average reward for episode 168.000.  Total average reward 140.913\n",
      "Average reward for episode 193.000.  Total average reward 141.434\n",
      "Average reward for episode 200.000.  Total average reward 142.020\n",
      "Average reward for episode 188.000.  Total average reward 142.480\n",
      "Average reward for episode 200.000.  Total average reward 143.055\n",
      "Average reward for episode 179.000.  Total average reward 143.414\n",
      "Average reward for episode 200.000.  Total average reward 143.980\n",
      "Average reward for episode 187.000.  Total average reward 144.410\n",
      "Average reward for episode 185.800.  Total average reward 144.824\n",
      "Average reward for episode 200.000.  Total average reward 145.376\n",
      "Average reward for episode 200.000.  Total average reward 145.922\n",
      "Average reward for episode 192.800.  Total average reward 146.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 194.600.  Total average reward 146.873\n",
      "Average reward for episode 200.000.  Total average reward 147.404\n",
      "Average reward for episode 200.000.  Total average reward 147.930\n",
      "Average reward for episode 200.000.  Total average reward 148.451\n",
      "Average reward for episode 200.000.  Total average reward 148.966\n",
      "Average reward for episode 197.800.  Total average reward 149.455\n",
      "Average reward for episode 200.000.  Total average reward 149.960\n",
      "Average reward for episode 193.400.  Total average reward 150.395\n",
      "Average reward for episode 178.800.  Total average reward 150.679\n",
      "Average reward for episode 182.400.  Total average reward 150.996\n",
      "Average reward for episode 199.800.  Total average reward 151.484\n",
      "Average reward for episode 194.400.  Total average reward 151.913\n",
      "Average reward for episode 200.000.  Total average reward 152.394\n",
      "Average reward for episode 200.000.  Total average reward 152.870\n",
      "Average reward for episode 200.000.  Total average reward 153.341\n",
      "Average reward for episode 188.200.  Total average reward 153.690\n",
      "Average reward for episode 170.400.  Total average reward 153.857\n",
      "Average reward for episode 200.000.  Total average reward 154.318\n",
      "Average reward for episode 200.000.  Total average reward 154.775\n",
      "Average reward for episode 191.600.  Total average reward 155.143\n",
      "Average reward for episode 200.000.  Total average reward 155.592\n",
      "Average reward for episode 200.000.  Total average reward 156.036\n",
      "Average reward for episode 167.000.  Total average reward 156.146\n",
      "Average reward for episode 200.000.  Total average reward 156.584\n",
      "Average reward for episode 200.000.  Total average reward 157.018\n",
      "Average reward for episode 200.000.  Total average reward 157.448\n",
      "Average reward for episode 200.000.  Total average reward 157.874\n",
      "Average reward for episode 200.000.  Total average reward 158.295\n",
      "Average reward for episode 200.000.  Total average reward 158.712\n",
      "Average reward for episode 200.000.  Total average reward 159.125\n",
      "Average reward for episode 200.000.  Total average reward 159.534\n",
      "Average reward for episode 200.000.  Total average reward 159.938\n",
      "Average reward for episode 199.800.  Total average reward 160.337\n",
      "Average reward for episode 200.000.  Total average reward 160.734\n",
      "Average reward for episode 200.000.  Total average reward 161.126\n",
      "Average reward for episode 200.000.  Total average reward 161.515\n",
      "Average reward for episode 200.000.  Total average reward 161.900\n",
      "Average reward for episode 200.000.  Total average reward 162.281\n",
      "Average reward for episode 200.000.  Total average reward 162.658\n",
      "Average reward for episode 200.000.  Total average reward 163.032\n",
      "Average reward for episode 200.000.  Total average reward 163.401\n",
      "Average reward for episode 200.000.  Total average reward 163.767\n",
      "Average reward for episode 184.000.  Total average reward 163.970\n",
      "Average reward for episode 200.000.  Total average reward 164.330\n",
      "Average reward for episode 185.400.  Total average reward 164.541\n",
      "Average reward for episode 199.000.  Total average reward 164.885\n",
      "Average reward for episode 198.400.  Total average reward 165.220\n",
      "Average reward for episode 200.000.  Total average reward 165.568\n",
      "Average reward for episode 176.000.  Total average reward 165.672\n",
      "Average reward for episode 200.000.  Total average reward 166.016\n",
      "Average reward for episode 200.000.  Total average reward 166.355\n",
      "Average reward for episode 200.000.  Total average reward 166.692\n",
      "Average reward for episode 200.000.  Total average reward 167.025\n",
      "Average reward for episode 200.000.  Total average reward 167.355\n",
      "Average reward for episode 200.000.  Total average reward 167.681\n",
      "Average reward for episode 180.000.  Total average reward 167.804\n",
      "Average reward for episode 200.000.  Total average reward 168.126\n",
      "Average reward for episode 200.000.  Total average reward 168.445\n",
      "Average reward for episode 171.600.  Total average reward 168.477\n",
      "Average reward for episode 200.000.  Total average reward 168.792\n",
      "Average reward for episode 177.800.  Total average reward 168.882\n",
      "Average reward for episode 185.600.  Total average reward 169.049\n",
      "Average reward for episode 199.800.  Total average reward 169.357\n",
      "Average reward for episode 190.400.  Total average reward 169.567\n",
      "Average reward for episode 188.800.  Total average reward 169.759\n",
      "Average reward for episode 198.600.  Total average reward 170.048\n",
      "Average reward for episode 194.600.  Total average reward 170.293\n",
      "Average reward for episode 200.000.  Total average reward 170.590\n",
      "Average reward for episode 168.400.  Total average reward 170.569\n",
      "Average reward for episode 190.400.  Total average reward 170.767\n",
      "Average reward for episode 200.000.  Total average reward 171.059\n",
      "Average reward for episode 192.200.  Total average reward 171.271\n",
      "Average reward for episode 178.800.  Total average reward 171.346\n",
      "Average reward for episode 192.200.  Total average reward 171.554\n",
      "Average reward for episode 193.200.  Total average reward 171.771\n",
      "Average reward for episode 200.000.  Total average reward 172.053\n",
      "Average reward for episode 151.600.  Total average reward 171.849\n",
      "Average reward for episode 200.000.  Total average reward 172.130\n",
      "Average reward for episode 190.800.  Total average reward 172.317\n",
      "Average reward for episode 180.400.  Total average reward 172.398\n",
      "Average reward for episode 200.000.  Total average reward 172.674\n",
      "Average reward for episode 200.000.  Total average reward 172.947\n",
      "Average reward for episode 197.200.  Total average reward 173.189\n",
      "Average reward for episode 191.200.  Total average reward 173.370\n",
      "Average reward for episode 193.200.  Total average reward 173.568\n",
      "Average reward for episode 184.400.  Total average reward 173.676\n",
      "Average reward for episode 200.000.  Total average reward 173.939\n",
      "Average reward for episode 186.600.  Total average reward 174.066\n",
      "Average reward for episode 200.000.  Total average reward 174.325\n",
      "Average reward for episode 192.400.  Total average reward 174.506\n",
      "Average reward for episode 200.000.  Total average reward 174.761\n",
      "Average reward for episode 185.600.  Total average reward 174.869\n",
      "Average reward for episode 200.000.  Total average reward 175.121\n",
      "Average reward for episode 200.000.  Total average reward 175.370\n",
      "Average reward for episode 172.600.  Total average reward 175.342\n",
      "Average reward for episode 195.800.  Total average reward 175.546\n",
      "Average reward for episode 196.000.  Total average reward 175.751\n",
      "Average reward for episode 200.000.  Total average reward 175.993\n",
      "Average reward for episode 200.000.  Total average reward 176.234\n",
      "Average reward for episode 184.200.  Total average reward 176.313\n",
      "Average reward for episode 200.000.  Total average reward 176.550\n",
      "Average reward for episode 194.600.  Total average reward 176.731\n",
      "Average reward for episode 189.000.  Total average reward 176.853\n",
      "Average reward for episode 200.000.  Total average reward 177.085\n",
      "Average reward for episode 186.200.  Total average reward 177.176\n",
      "Average reward for episode 200.000.  Total average reward 177.404\n",
      "Average reward for episode 200.000.  Total average reward 177.630\n",
      "Average reward for episode 183.400.  Total average reward 177.688\n",
      "Average reward for episode 200.000.  Total average reward 177.911\n",
      "Average reward for episode 195.600.  Total average reward 178.088\n",
      "Average reward for episode 200.000.  Total average reward 178.307\n",
      "Average reward for episode 200.000.  Total average reward 178.524\n",
      "Average reward for episode 199.800.  Total average reward 178.737\n",
      "Average reward for episode 200.000.  Total average reward 178.949\n",
      "Average reward for episode 200.000.  Total average reward 179.160\n",
      "Average reward for episode 193.400.  Total average reward 179.302\n",
      "Average reward for episode 200.000.  Total average reward 179.509\n",
      "Average reward for episode 200.000.  Total average reward 179.714\n",
      "Average reward for episode 198.800.  Total average reward 179.905\n",
      "Average reward for episode 164.800.  Total average reward 179.754\n",
      "Average reward for episode 200.000.  Total average reward 179.956\n",
      "Average reward for episode 200.000.  Total average reward 180.157\n",
      "Average reward for episode 200.000.  Total average reward 180.355\n",
      "Average reward for episode 199.200.  Total average reward 180.544\n",
      "Average reward for episode 190.200.  Total average reward 180.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 189.200.  Total average reward 180.726\n",
      "Average reward for episode 191.800.  Total average reward 180.837\n",
      "Average reward for episode 200.000.  Total average reward 181.028\n",
      "Average reward for episode 200.000.  Total average reward 181.218\n",
      "Average reward for episode 192.200.  Total average reward 181.328\n",
      "Average reward for episode 200.000.  Total average reward 181.514\n",
      "Average reward for episode 200.000.  Total average reward 181.699\n",
      "Average reward for episode 200.000.  Total average reward 181.882\n",
      "Average reward for episode 200.000.  Total average reward 182.063\n",
      "Average reward for episode 200.000.  Total average reward 182.243\n",
      "Average reward for episode 187.000.  Total average reward 182.290\n",
      "Average reward for episode 200.000.  Total average reward 182.468\n",
      "Average reward for episode 200.000.  Total average reward 182.643\n",
      "Average reward for episode 199.400.  Total average reward 182.810\n",
      "Average reward for episode 200.000.  Total average reward 182.982\n",
      "Average reward for episode 188.600.  Total average reward 183.038\n",
      "Average reward for episode 200.000.  Total average reward 183.208\n",
      "Average reward for episode 200.000.  Total average reward 183.376\n",
      "Average reward for episode 200.000.  Total average reward 183.542\n",
      "Average reward for episode 200.000.  Total average reward 183.707\n",
      "Average reward for episode 200.000.  Total average reward 183.870\n",
      "Average reward for episode 200.000.  Total average reward 184.031\n",
      "Average reward for episode 200.000.  Total average reward 184.191\n",
      "Average reward for episode 200.000.  Total average reward 184.349\n",
      "Average reward for episode 200.000.  Total average reward 184.505\n",
      "Average reward for episode 200.000.  Total average reward 184.660\n",
      "Average reward for episode 178.200.  Total average reward 184.596\n",
      "Average reward for episode 188.600.  Total average reward 184.636\n",
      "Average reward for episode 200.000.  Total average reward 184.789\n",
      "Average reward for episode 200.000.  Total average reward 184.941\n",
      "Average reward for episode 187.600.  Total average reward 184.968\n",
      "Average reward for episode 183.600.  Total average reward 184.954\n",
      "Average reward for episode 196.400.  Total average reward 185.069\n",
      "Average reward for episode 177.800.  Total average reward 184.996\n",
      "Average reward for episode 178.600.  Total average reward 184.932\n",
      "Average reward for episode 200.000.  Total average reward 185.083\n",
      "Average reward for episode 192.600.  Total average reward 185.158\n",
      "Average reward for episode 191.200.  Total average reward 185.218\n",
      "Average reward for episode 200.000.  Total average reward 185.366\n",
      "Average reward for episode 199.800.  Total average reward 185.511\n",
      "Average reward for episode 200.000.  Total average reward 185.656\n",
      "Average reward for episode 200.000.  Total average reward 185.799\n",
      "Average reward for episode 189.400.  Total average reward 185.835\n",
      "Average reward for episode 200.000.  Total average reward 185.977\n",
      "Average reward for episode 195.000.  Total average reward 186.067\n",
      "Average reward for episode 198.200.  Total average reward 186.188\n",
      "Average reward for episode 200.000.  Total average reward 186.326\n",
      "Average reward for episode 193.000.  Total average reward 186.393\n",
      "Average reward for episode 200.000.  Total average reward 186.529\n",
      "Average reward for episode 200.000.  Total average reward 186.664\n",
      "Average reward for episode 200.000.  Total average reward 186.797\n",
      "Average reward for episode 200.000.  Total average reward 186.929\n",
      "Average reward for episode 200.000.  Total average reward 187.060\n",
      "Average reward for episode 193.800.  Total average reward 187.127\n",
      "Average reward for episode 200.000.  Total average reward 187.256\n",
      "Average reward for episode 200.000.  Total average reward 187.383\n",
      "Average reward for episode 200.000.  Total average reward 187.510\n",
      "Average reward for episode 171.600.  Total average reward 187.351\n",
      "Average reward for episode 200.000.  Total average reward 187.477\n",
      "Average reward for episode 198.200.  Total average reward 187.584\n",
      "Average reward for episode 200.000.  Total average reward 187.708\n",
      "Average reward for episode 174.400.  Total average reward 187.575\n",
      "Average reward for episode 200.000.  Total average reward 187.700\n",
      "Average reward for episode 200.000.  Total average reward 187.823\n",
      "Average reward for episode 200.000.  Total average reward 187.944\n",
      "Average reward for episode 200.000.  Total average reward 188.065\n",
      "Average reward for episode 193.800.  Total average reward 188.122\n",
      "Average reward for episode 200.000.  Total average reward 188.241\n",
      "Average reward for episode 200.000.  Total average reward 188.359\n",
      "Average reward for episode 200.000.  Total average reward 188.475\n",
      "Average reward for episode 200.000.  Total average reward 188.590\n",
      "Average reward for episode 200.000.  Total average reward 188.704\n",
      "Average reward for episode 200.000.  Total average reward 188.817\n",
      "Average reward for episode 200.000.  Total average reward 188.929\n",
      "Average reward for episode 200.000.  Total average reward 189.040\n",
      "Average reward for episode 200.000.  Total average reward 189.150\n",
      "Average reward for episode 196.600.  Total average reward 189.224\n",
      "Average reward for episode 200.000.  Total average reward 189.332\n",
      "Average reward for episode 200.000.  Total average reward 189.438\n",
      "Average reward for episode 200.000.  Total average reward 189.544\n",
      "Average reward for episode 200.000.  Total average reward 189.649\n",
      "Average reward for episode 200.000.  Total average reward 189.752\n",
      "Average reward for episode 200.000.  Total average reward 189.855\n",
      "Average reward for episode 200.000.  Total average reward 189.956\n",
      "Average reward for episode 200.000.  Total average reward 190.057\n",
      "Average reward for episode 200.000.  Total average reward 190.156\n",
      "Average reward for episode 200.000.  Total average reward 190.254\n",
      "Average reward for episode 197.600.  Total average reward 190.328\n",
      "Average reward for episode 200.000.  Total average reward 190.425\n",
      "Average reward for episode 200.000.  Total average reward 190.520\n",
      "Average reward for episode 196.800.  Total average reward 190.583\n",
      "Average reward for episode 200.000.  Total average reward 190.677\n",
      "Average reward for episode 200.000.  Total average reward 190.771\n",
      "Average reward for episode 200.000.  Total average reward 190.863\n",
      "Average reward for episode 200.000.  Total average reward 190.954\n",
      "Average reward for episode 200.000.  Total average reward 191.045\n",
      "Average reward for episode 200.000.  Total average reward 191.134\n",
      "Average reward for episode 200.000.  Total average reward 191.223\n",
      "Average reward for episode 200.000.  Total average reward 191.311\n",
      "Average reward for episode 200.000.  Total average reward 191.398\n",
      "Average reward for episode 163.800.  Total average reward 191.122\n",
      "Average reward for episode 200.000.  Total average reward 191.210\n",
      "Average reward for episode 200.000.  Total average reward 191.298\n",
      "Average reward for episode 200.000.  Total average reward 191.385\n",
      "Average reward for episode 200.000.  Total average reward 191.471\n",
      "Average reward for episode 200.000.  Total average reward 191.557\n",
      "Average reward for episode 177.800.  Total average reward 191.419\n",
      "Average reward for episode 200.000.  Total average reward 191.505\n",
      "Average reward for episode 179.400.  Total average reward 191.384\n",
      "Average reward for episode 200.000.  Total average reward 191.470\n",
      "Average reward for episode 200.000.  Total average reward 191.555\n",
      "Average reward for episode 200.000.  Total average reward 191.640\n",
      "Average reward for episode 200.000.  Total average reward 191.723\n",
      "Average reward for episode 200.000.  Total average reward 191.806\n",
      "Average reward for episode 200.000.  Total average reward 191.888\n",
      "Average reward for episode 200.000.  Total average reward 191.969\n",
      "Average reward for episode 200.000.  Total average reward 192.050\n",
      "Average reward for episode 200.000.  Total average reward 192.129\n",
      "Average reward for episode 200.000.  Total average reward 192.208\n",
      "Average reward for episode 200.000.  Total average reward 192.286\n",
      "Average reward for episode 200.000.  Total average reward 192.363\n",
      "Average reward for episode 200.000.  Total average reward 192.439\n",
      "Average reward for episode 172.400.  Total average reward 192.239\n",
      "Average reward for episode 200.000.  Total average reward 192.316\n",
      "Average reward for episode 200.000.  Total average reward 192.393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 200.000.  Total average reward 192.469\n",
      "Average reward for episode 198.400.  Total average reward 192.529\n",
      "Average reward for episode 200.000.  Total average reward 192.603\n",
      "Average reward for episode 200.000.  Total average reward 192.677\n",
      "Average reward for episode 200.000.  Total average reward 192.750\n",
      "Average reward for episode 193.400.  Total average reward 192.757\n",
      "Average reward for episode 200.000.  Total average reward 192.829\n",
      "Average reward for episode 200.000.  Total average reward 192.901\n",
      "Average reward for episode 200.000.  Total average reward 192.972\n",
      "Average reward for episode 200.000.  Total average reward 193.042\n",
      "Average reward for episode 200.000.  Total average reward 193.112\n",
      "Average reward for episode 200.000.  Total average reward 193.181\n",
      "Average reward for episode 200.000.  Total average reward 193.249\n",
      "Average reward for episode 200.000.  Total average reward 193.317\n",
      "Average reward for episode 200.000.  Total average reward 193.383\n",
      "Average reward for episode 200.000.  Total average reward 193.450\n",
      "Average reward for episode 200.000.  Total average reward 193.515\n",
      "Average reward for episode 200.000.  Total average reward 193.580\n",
      "Average reward for episode 200.000.  Total average reward 193.644\n",
      "Average reward for episode 200.000.  Total average reward 193.708\n",
      "Average reward for episode 200.000.  Total average reward 193.771\n",
      "Average reward for episode 200.000.  Total average reward 193.833\n",
      "Average reward for episode 200.000.  Total average reward 193.895\n",
      "Average reward for episode 200.000.  Total average reward 193.956\n",
      "Average reward for episode 200.000.  Total average reward 194.016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-85714c0e1a2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# 위에서 만들어둔 policy network를 실행하고 수행할 작업을 가져와라.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtfprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#학습이 진행됨.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtfprob\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\redstone\\anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\redstone\\anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\users\\redstone\\anaconda3\\envs\\cuda\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.global_variables_initializer() # 모든 변수 초기화\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # cartpole 환경을 초기화\n",
    "\n",
    "    # gradient placeholder를 리셋한다.\n",
    "    # 우리는 정책 네트워크를 업데이트할 준비가 될 때까지 GradBuffer의 기울기를 수집할 것이다.\n",
    "    gradBuffer = sess.run(tvars)  #gradient buffer에 weight 값을 넣어라.\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:         \n",
    "        # 환경을 조성하면서 학습을 진행하면 학습 속도가 느려지기 때문에 어느정도 성능이 높아진다면,\n",
    "        # 아래 if문의 코드를 실행하여 cartpole 환경을 보여준다.\n",
    "        # 처음에는 안보임.\n",
    "        if reward_sum/batch_size > 100 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # 임의의 값을 넣어서 나오는 결과값들(관측치)이 네트워크가 처리할 수 있는 모양으로 만들어라.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # 위에서 만들어둔 policy network를 실행하고 수행할 작업을 가져와라.\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x}) #학습이 진행됨.\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        # 진행되면서 생기는 (나중에 역전파를 위해 필요한 다양한) 중간 변수를 기록한다.\n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # cartpole 시뮬레이션을 진행하고, 이를통해 새로운 측정치를 얻는다.\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        # reward를 기록. \n",
    "        # 이전 action에 대한 보상을 받기 위해 반드시 step()을 호출한 뒤 수행해야한다.\n",
    "        drs.append(reward) \n",
    "        \n",
    "        if done: \n",
    "            # 다음 에피소드로 가기위해 변수에 +1\n",
    "            episode_number += 1\n",
    "            # 현재 에피소드의 모든 입력값, 은닉 상태, 액션 경사, 보상을 쌓는다.\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            # 배열 메모리를 리셋한다.\n",
    "            xs,drs,ys = [],[],[]\n",
    "\n",
    "            # 학습이 진행되면서 생긴 discounted reward를 역으로 계산한다.\n",
    "            # reward의 크기를 표준화한다.(표준화 방법 = 평균을 빼주고 표준편차로 나눠준다.)\n",
    "            # cartpole은 항상 1의 보상을 받기 때문에 모든 action에 대한 return값이 항상 0보다 크다.\n",
    "            # 따라서 그대로 return을 사용하면 모든 action을 수용하게되고 그 정도에만 차이가 생기게 된다.\n",
    "            # 하지만 이렇게 표준화한다면 변화가 커서 수렴을 잘 안하기 때문에 이 방법을 쓴다.\n",
    "            # 표준화를 통해 좋지 못한 action은 버리고 좋은 action만을 수용하기 때문에 수학적으로 변화가 줄어든다.\n",
    "            # 즉, 정규화 ((reward-mean)/std)가 일종의 advatage function이 될 수 있음.\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # 이번 에피소드의 gradient를 얻고 gradBuffer에 gradient를 저장하라.\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # batch_size만큼 에피소드를 완료했다면, 정책 네트워크를 우리가 얻은 gradient와 함께 갱신한다.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # 학습이 어떻게 진행됐는지 확인한다.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode %.3f.  Total average reward %.3f' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                # 만약 reward_sum/batch_size이 결과가 200보다 크다면 cartpole 예제가 해결된것이다.\n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print(\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "print(episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
